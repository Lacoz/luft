[core]
# Core settings.
########
env = DEV
# Default tasks folder. Or you can use TASK_FOLDER enviromental variable.
tasks_folder = example/tasks
# JDBC config file location. Or you can use JDBC_CONFIG enviromental variable.
jdbc_config = example/config/jdbc.cfg
# Blob storage for temporary saving data (datalake). Or set BLOB_STORAGE.
# Currently only aws (s3) and gcs (Google cloud storage) are supported.
blob_storage = aws
# Path prefix (path) on blob storage. You can use following templated fields:
# {env} - your environment (DEV/PROD...)
# {source_system} - name of source system (whatever you like) - in case of jdbc it is usually friendly name of db
# {source_subsystem} - name of source subsystem (whatever you like) - in case of jdbc it is schema name
# {name} - name of table
# {date_valid} - date of valid of export
# {time_valid} - time valid of export
path_prefix = {env}/{source_system}/{source_subsystem}/{name}/{date_valid}/{time_valid}/data

[bq]
# BigQuery settings
########
# Service account credentials file (mandatory). Or set BQ_CREDENTIALS_FILE
credentials_file = 
# BigQuery default project (mandatory). Or set BQ_PROJECT_ID.
project_id = 
# Big Query data location - https://cloud.google.com/bigquery/docs/locations. Or set BQ_LOCATION.
location = 
# The prefix to use for a randomly generated job ID. Or set BQ_JOB_ID_PREFIX.
job_id_prefix = luft-
# Default history template
default_history_template = templates/sql/bq/history_change_only.sql
# Default stage template
default_stage_template = templates/sql/bq/create_stage_table.sql
# Name of dataset_id for staging. You can use following templated fields:
# {env} - your environment (DEV/PROD...)
# {source_system} - name of source system (whatever you like) - in case of jdbc it is usually friendly name of db
# {source_subsystem} - name of source subsystem (whatever you like) - in case of jdbc it is schema name
# {dataset_id} - identifier of dataset - usually same as source_system
stage_schema_form = stage_{dataset_id}


[gcs]
# Google cloud cloud storage settings
########
# Google Cloud Storage bucket name. Or you can set GCS_BUCKET.
bucket = 
# authorization methods - private_key, json_key. Or you can set GCS_AUTH_METHOD.
auth_method = json_key
# Applicable only for private_key authorization method.
# Or you can set GCS_EMAIL and GCS_P12_KEYFILE.
service_account_email =
p12_keyfile = 
# Applicable only for json_key authorization method. Or you can set GCS_JSON_KEYFILE
# It can be full path to json keyfile or you can embed content of json_keyfile like:
# json_keyfile = 
#   content: |
#      {
#          "private_key_id": "123456789",
#          "private_key": "-----BEGIN PRIVATE KEY-----\nABCDEF",
#          "client_email": "..."
#       }
json_keyfile = 
# Application name, anything you like (default value is "embulk-output-gcs"). Or you can set GCS_APP_NAME.
application_name = 

[aws]
# AWS S3 storage settings
########
# S3 bucket name. Or you can set AWS_BUCKET.
bucket =
# S3 endpoint login user name. Or you can set AWS_ENDPOINT.
endpoint =
# AWS access key id. Or you can set AWS_ACCESS_KEY_ID.
access_key_id =
# AWS secret key. Or you can set AWS_SECRET_ACCESS_KEY.
secret_access_key =

[task_type_map]
# Mapping of task name into Marshmallow schema.
embulk-jdbc-load = luft.schemas.embulk_jdbc_task_schema.EmbulkJdbcTaskSchema
bq-exec = luft.schemas.bq_exec_task_schema.BQExecTaskSchema
bq-load = luft.schemas.bq_load_task_schema.BQLoadTaskSchema
qlik-cloud-upload = luft.schemas.qlik_cloud_task_schema.QlikCloudTaskSchema

[embulk]
# Embulk command. Change path if necessary.
# embulk_command = java -jar /opt/embulk/embulk.jar
embulk_command = java -jar /opt/embulk/embulk.jar
# embulk log level
embulk_log_level = debug

[embulk_default_template]
# Default Embulk templates. Templates are installed along with luft (are inside package).
# Template name is formated with {blob_storage} variable. Where variable is from [core]
# For every task you can change your embulk template by setting embulk_template: <full_path>
embulk-jdbc-load = templates/embulk/jdbc_{blob_storage}.yml.liquid

[embulk_type_map]
# Mapping Sql type to Embulk type. List of supported Embulk data types:
# https://www.embulk.org/docs/built-in.html#id2

# If you have any special data type in your database change it here.
string = string
number = long
numeric = long
int64 = long
variant = string
float = double
float64 = double
boolean = boolean
bool = boolean
struct = string
record = string
array = string
date = string, timestamp_format: '%%Y-%%m-%%d'
time = string, timestamp_format: '%%H:%%M:%%S %%z'
datetime = string, timestamp_format: '%%Y-%%m-%%d %%H:%%M:%%S %%z'
timestamp = string, timestamp_format: '%%Y-%%m-%%d %%H:%%M:%%S %%z'
timestamp_ntz = string, timestamp_format: '%%Y-%%m-%%d %%H:%%M:%%S'
timestamp_ltz = string, timestamp_format: '%%Y-%%m-%%d %%H:%%M:%%S %%z'
timestamp_tz = string, timestamp_format: '%%Y-%%m-%%d %%H:%%M:%%S %%z'

[qlik_cloud]
# Delay of waiting for Element to appear in second
delay = 10
# Qlik Sense Cloud Login url
login_url = https://qlikid.qlik.com/cloud
# Qlik Sense Cloud main site
url = https://eu.qlikcloud.com/hub
# Qlik Sense Username
user =
# Qlik Sense User Password
password =
# Run as headless browser. It will run without any GUI if you set Headless to True.
headless = False

[qlik_enterprise]
# Qlik Enterprise settings
# Qlik Sense 
host =
port = 4242
client_cert =
client_key =
root_cert =

[jdbc_driver_path]
# postgresql =  postgresql-42.0.0.jar
# sqlserver =  sqljdbc42.jar
# mysql =  mysql-connector-java-5.1.40-bin.jar
# db2 =  db2jcc4.jar
# oracle =  ojdbc7.jar

[thread]
# Thread is the basic unit of parallelism in Airflow. Imagine you would like to import 4 tables -
# Table1, Table2, Table3, Table4. Without any threads (parallelism) it would take approx. 151m.
# (Table1 - 15m, Table2 - 70m, Table3 - 15m, Table4 - 51m). If you would have 2 threads like this:

# - Thread1 - Table2 (70m)
# - Thread2 - Table1 -> Table2 -> Table3 (81m)
# Whole report will therefore take only 81m.

# Every generated thread has prefix - it has meaning only if you specify thread in your yml file.
thread_prefix = thread_gen-

# Default thread count
default_thread_cnt = 3

[data_type]
# List of supported data types in your yaml definition. If you need some other or want to disable
# some of them just edit this part.
bq = 
    INT64
    INTEGER
    NUMERIC
    FLOAT64
    STRING
    BOOLEAN
    BOOL
    TIMESTAMP
    BYTES
    DATE
    TIME
    DATETIME
    ARRAY
    STRUCT
    RECORD


snowflake = 
    ARRAY
    BIGINT
    BINARY
    BOOL
    BOOLEAN
    BYTES
    CHAR
    CHARACTER
    DATE
    DATETIME
    DECIMAL
    DOUBLE
    DOUBLE PRECISION
    FLOAT
    FLOAT4
    FLOAT64
    FLOAT8
    INT
    INT64
    INTEGER
    NUMBER
    NUMERIC
    OBJECT
    REAL
    RECORD
    SMALLINT
    STRING
    STRUCT
    TEXT
    TIME
    TIMESTAMP
    TIMESTAMP_LTZ
    TIMESTAMP_NTZ
    TIMESTAMP_TZ
    VARBINARY
    VARCHAR
    VARIANT